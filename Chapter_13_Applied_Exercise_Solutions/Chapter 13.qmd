---
title: "Multiple Testing"
subtitle: "Chapter 13: ISLR2 Exercise Solutions"
author: "Shad Ali Shah"
date: today
format:
  html:
    toc: true
    toc-depth: 2
    toc-location: left
    number-sections: true
    number-depth: 3
    code-fold: true
    code-tools: true
    code-summary: "Show code"
    theme: cosmo
    colorlinks: true
    link-citations: true
    df-print: kable
    fig-cap-location: bottom
    tbl-cap-location: top
    crossref:
      fig-title: "Figure"
      tbl-title: "Table"
      fig-prefix: "Fig."
      tbl-prefix: "Table"
execute:
  echo: true
  warning: false
  message: false
  error: true
  cache: false
  keep-going: true
  fig-width: 6.5
  fig-height: 4
jupyter: python3
---

# Chapter 13

```{python}
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import statsmodels.api as sm
from ISLP import load_data
from ISLP.models import (ModelSpec as MS, summarize)
from scipy.stats import \
    (ttest_1samp,
     ttest_rel,
     ttest_ind,
     t as t_dbn)
from statsmodels.stats.multicomp import \
     pairwise_tukeyhsd
from statsmodels.stats.multitest import \
     multipletests as mult_test
```

## Exercise 7

```{python}
Carseats=load_data('Carseats')
```

### (a)

```{python}
Carseats.head()
```

```{python}
Carseats.info()
```

```{python}
design=MS(Carseats.columns.drop(['Sales','ShelveLoc','Urban','US']))
y=Carseats['Sales']
X=design.fit_transform(Carseats)
X

```

```{python}
Xs=[]
results=[]
p_values=[]
for i in range(1,X.shape[1]):
    Xs.append(X.iloc[:,[0,i]])
for i in range(len(Xs)):
    results.append(sm.OLS(y,Xs[i]).fit())
for i in range(len(results)):
    p_values.append(results[i].pvalues[1])#This way you will prevent pvalues form being rounded
df=pd.DataFrame({'Variables':X.columns.drop(['intercept']),'p_values':p_values})
```

```{python}
df
```

### (b)

If we control the type I error for each the seven models fitted in the previous question at level 5%, then we reject the null hypotheses of the form $H_{0,j}: \beta_{1,j}=0$, associated with variables 'CompPrice', 'Population' and 'Education'.

### (c)

```{python}
#We use the Holm's step down approach 
alpha=0.05
m=df.shape[0]
df_sorted=df.sort_values(by=['p_values'])
#Step 4 of Holm's step down method
condition=[]
for i in range(df_sorted.shape[0]):
    if df_sorted['p_values'].iloc[i]>alpha/(m+2-i):
        condition.append(i)

```

```{python}
L=min(condition)
```

```{python}
df_sorted['Reject']=np.where(df_sorted['p_values']<df_sorted['p_values'].iloc[L],'Yes','No')
```

```{python}
df_sorted
```

### (d)

```{python}
qvalues = mult_test(df['p_values'],alpha=0.2, method = "fdr_bh")[0]
```

```{python}
df['Reject_FDR_bh']=np.where(qvalues==True,'Yes','No')
```

```{python}
df
```

## Exercise 8

```{python}
rng = np.random.default_rng (1)
n, m = 20, 100
X = rng.normal(size=(n, m))
```

```{python}
X.shape
```

### (a)

```{python}
results = ttest_1samp(X, 0).pvalue
```

```{python}
results.shape
```

```{python}
kwargs = dict(histtype='bar', density=True, ec="k")
plt.hist(results,**kwargs)
plt.title('Histogram of p-values')
plt.xlabel('p-values')
plt.ylabel('Count');
```

The histogram show a relatively flat distribution. This stems from the fact that all null hypotheses are true.  
It also invokes that controlling Type I error for each null hypothesis using a unified significance level, as prescribed in the following question, is a bad idea.

### (b)

```{python}
(results<0.05).sum()
```

We reject 4 null hypotheses.

### (c)

```{python}
reject, holm = mult_test(results, method = "holm")[:2]
reject
```

By using Holm step down method, we have failed to reject all null hypotheses.

### (d)

```{python}
reject_fdr=mult_test(results, method = "fdr_bh")[0]
reject_fdr
```

As the previous question, we have failed to reject all null hypotheses.

### (e)

```{python}
#Picking the 10 managers who perform well. We consider that performant managers are those whose 
# whose returns are not due to chance; i.e: those with the smallest p-values 
performance=[]
for i in range(X.shape[1]):
    performance.append((results[i].sum(),i))
performance.sort(key=lambda x: x[0], reverse=False)
```

```{python}
#Top 10 fund managers
top_10=performance[:10]
top_10_pvals=[]
for i in range(10):
    top_10_pvals.append(results[top_10[i][1]])
```

```{python}
top_10
```

```{python}
#Controlling FWER of the top 10 managers
reject_cp= mult_test(top_10_pvals, method = "holm")[:2]
reject_cp
```

```{python}
#Controlling FDR of the top 10 managers
reject_cp_fdr= mult_test(top_10_pvals, method = "fdr_bh")[:2]
reject_cp_fdr
```

We reject the null hypothesis corresponding to manager no 14 (Indexes start from 0 to 99), when controlling each of FWER and FDR.

### (f)

The approach of "cherry-picking" when controlling FDR or FWER is misleading.

This is because control algorithms (e.g: Holm's step down method or Benjamini-Hochberg) depend on the number of null hypotheses to determine the level of significance.

When we cherry-pick we tend to only show what we want, which is in fact a dangerous cognitive bias. For example, here we have falsely showed that the best performant manager owes his achieved returns only to chance.

```{python}

```
